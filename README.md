# ğŸš€ Proyecto de AutomatizaciÃ³n HÃ­brida (UI & API) con Pytest

Este proyecto implementa un *framework* de automatizaciÃ³n de pruebas **hÃ­brido** (Interfaz de Usuario y API) utilizando **Python** y **Pytest**. El objetivo es validar la funcionalidad crÃ­tica de la aplicaciÃ³n web **SauceDemo** (UI) y probar un *mock* de servicio API externo (**ReqRes**).

## ğŸ’¡ PropÃ³sito y CaracterÃ­sticas Destacadas

El framework estÃ¡ diseÃ±ado para demostrar las mejores prÃ¡cticas en automatizaciÃ³n, cubriendo un ecosistema completo de pruebas:

*   **Arquitectura POM (Page Object Model):** CÃ³digo modular y reutilizable para las pruebas de UI (SauceDemo).
*   **Service Object Pattern:** LÃ³gica separada y clara para las pruebas de API.
*   **Data Driven Testing (DDT):** Las pruebas de Login y CatÃ¡logo utilizan datos externos cargados desde archivos **CSV** y **JSON**.
*   **Flujos E2E y Encadenamiento:** Cobertura de flujos completos de compra (UI) y flujos encadenados de creaciÃ³n/eliminaciÃ³n de recursos (API).
*   **Reporting Profesional:** GeneraciÃ³n de reportes detallados en HTML con **capturas de pantalla automÃ¡ticas** en caso de fallo.
*   **Logging:** Sistema de registro detallado (`execution.log`) para facilitar la depuraciÃ³n de los pasos de ejecuciÃ³n.

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| TecnologÃ­a | Rol |
| :--- | :--- |
| **Python 3.12+** | Lenguaje de programaciÃ³n principal. |
| **Pytest** | Marco de pruebas (framework) principal para la ejecuciÃ³n y gestiÃ³n de fixtures. |
| **Selenium** | AutomatizaciÃ³n de la Interfaz de Usuario (UI). |
| **Requests** | Manejo de peticiones HTTP (pruebas de API). |
| **Pytest-HTML** | Plugin para la generaciÃ³n de reportes avanzados. |
| **WebDriver Manager** | GestiÃ³n automÃ¡tica del *driver* del navegador (Chrome). |

---

## ğŸ“‚ Estructura del Proyecto

La estructura sigue el patrÃ³n de separaciÃ³n de responsabilidades:

```text
proyecto-final-automation-testing
â”œâ”€â”€ data/                    # Archivos de datos externos (DDT)
â”‚   â”œâ”€â”€ inventory.json       # Datos de productos para el CatÃ¡logo.
â”‚   â””â”€â”€ users.csv            # Usuarios y contraseÃ±as para Login.
â”œâ”€â”€ pages/                   # MÃ³dulos del Page Object Model (UI)
â”‚   â”œâ”€â”€ login_page.py        # Page Object para Login.
â”‚   â”œâ”€â”€ inventory_page.py    # Page Object para Inventario.
â”‚   â””â”€â”€ ...                  # (Otros Page Objects)
â”œâ”€â”€ services/                # MÃ³dulos del Service Object Pattern (API)
â”‚   â””â”€â”€ reqres_service.py    # LÃ³gica de la API ReqRes.
â”œâ”€â”€ Tests/                   # Archivos de Tests
â”‚   â”œâ”€â”€ test_api_reqres.py   # Tests del backend.
â”‚   â”œâ”€â”€ test_login.py        # Tests de login (incluye DDT).
â”‚   â””â”€â”€ ...                  # (Otros tests de UI)
â”œâ”€â”€ utils/                   # Utilidades del Framework
â”‚   â””â”€â”€ data_loader.py       # LÃ³gica para leer CSV/JSON.
â”œâ”€â”€ conftest.py              # Configuraciones globales (Fixtures, Hooks, Logging).
â”œâ”€â”€ screenshots/             # Carpeta de salida para capturas de fallo.
â”œâ”€â”€ execution.log            # Archivo de registro generado.
â”œâ”€â”€ requirements.txt         # Lista de dependencias del proyecto.
â””â”€â”€ README.md                # Este documento.
```

---

## âš™ï¸ Â¿CÃ³mo Instalar las Dependencias?

AsegÃºrate de tener **Python 3.12** o superior instalado. Luego, usa `pip` para instalar todas las librerÃ­as necesarias ejecutando el siguiente comando en la raÃ­z del proyecto:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Â¿CÃ³mo Ejecutar las Pruebas?

Las pruebas se ejecutan utilizando el comando `pytest` con marcadores (`-m`) para seleccionar los flujos deseados.

### 1. EjecuciÃ³n Completa con Reporte y Logging (Recomendado)

Este comando ejecuta todos los tests de UI y API, genera el reporte HTML e inicia el sistema de logging:

```bash
pytest -v --html=reporte_final_ejecucion.html --self-contained-html
```

### 2. EjecuciÃ³n Selectiva por Marcador

Puedes ejecutar grupos especÃ­ficos de pruebas utilizando los marcadores definidos en `pytest.ini`:

| Comando | DescripciÃ³n |
| :--- | :--- |
| `pytest -m api -v` | Ejecuta solo las pruebas de la API (backend). |
| `pytest -m login -v` | Ejecuta solo las pruebas de Login (incluyendo DDT). |
| `pytest -m smoke -v` | Ejecuta solo el conjunto de pruebas crÃ­ticas. |
| `pytest -m regression -v` | Ejecuta todas las pruebas de regresiÃ³n. |

---

## ğŸ“Š Â¿CÃ³mo Interpretar los Reportes Generados?

### Reporte HTML (`reporte_final_ejecucion.html`)
Este archivo se genera en la raÃ­z del proyecto tras la ejecuciÃ³n.
*   **Estado:** Muestra claramente el resultado de cada prueba (`PASSED`, `FAILED`, `ERROR`).
*   **Evidencia:** Si una prueba de UI falla, la **captura de pantalla** se adjunta directamente en la secciÃ³n "Extra" del reporte HTML, ademÃ¡s de guardarse en la carpeta `screenshots/`.
*   **Tiempos:** Indica la duraciÃ³n de cada test y la duraciÃ³n total de la ejecuciÃ³n.

### Archivo de Logging (`execution.log`)
Este archivo contiene el historial detallado de la ejecuciÃ³n.
*   **Formato:** Muestra la fecha, el nivel (`INFO`, `WARNING`, `ERROR`), el mÃ³dulo y el mensaje.
*   **Uso en DepuraciÃ³n:** Si una prueba falla, revisa el `execution.log` para ver el Ãºltimo paso exitoso registrado por el Page Object o la fixture antes del error.